{"cells":[{"cell_type":"markdown","source":["For help, look here:\nhttps://spark.apache.org/docs/latest/rdd-programming-guide.html"],"metadata":{}},{"cell_type":"code","source":["# Check out pre-loaded dataset\ndisplay(dbutils.fs.ls('dbfs:/'))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/</td><td>FileStore/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-results/</td><td>databricks-results/</td><td>0</td></tr><tr><td>dbfs:/tmp/</td><td>tmp/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Create a rdd (sc = SparkContext)\nrdd = sc.textFile(\"dbfs:/databricks-datasets/SPARK_README.md\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Read 20 lines \nrdd.take(20)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [&#39;# Apache Spark&#39;,\n &#39;&#39;,\n &#39;Spark is a fast and general cluster computing system for Big Data. It provides&#39;,\n &#39;high-level APIs in Scala, Java, Python, and R, and an optimized engine that&#39;,\n &#39;supports general computation graphs for data analysis. It also supports a&#39;,\n &#39;rich set of higher-level tools including Spark SQL for SQL and DataFrames,&#39;,\n &#39;MLlib for machine learning, GraphX for graph processing,&#39;,\n &#39;and Spark Streaming for stream processing.&#39;,\n &#39;&#39;,\n &#39;&lt;http://spark.apache.org/&gt;&#39;,\n &#39;&#39;,\n &#39;&#39;,\n &#39;## Online Documentation&#39;,\n &#39;&#39;,\n &#39;You can find the latest Spark documentation, including a programming&#39;,\n &#39;guide, on the [project web page](http://spark.apache.org/documentation.html)&#39;,\n &#39;and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).&#39;,\n &#39;This README file only contains basic setup instructions.&#39;,\n &#39;&#39;,\n &#39;## Building Spark&#39;]</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Example: lambda functions  \nwords = rdd.flatMap(lambda lines: lines.split(\" \"))\n\nfor w in words.collect():\n  print(w)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">#\nApache\nSpark\n\nSpark\nis\na\nfast\nand\ngeneral\ncluster\ncomputing\nsystem\nfor\nBig\nData.\nIt\nprovides\nhigh-level\nAPIs\nin\nScala,\nJava,\nPython,\nand\nR,\nand\nan\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis.\nIt\nalso\nsupports\na\nrich\nset\nof\nhigher-level\ntools\nincluding\nSpark\nSQL\nfor\nSQL\nand\nDataFrames,\nMLlib\nfor\nmachine\nlearning,\nGraphX\nfor\ngraph\nprocessing,\nand\nSpark\nStreaming\nfor\nstream\nprocessing.\n\n&lt;http://spark.apache.org/&gt;\n\n\n##\nOnline\nDocumentation\n\nYou\ncan\nfind\nthe\nlatest\nSpark\ndocumentation,\nincluding\na\nprogramming\nguide,\non\nthe\n[project\nweb\npage](http://spark.apache.org/documentation.html)\nand\n[project\nwiki](https://cwiki.apache.org/confluence/display/SPARK).\nThis\nREADME\nfile\nonly\ncontains\nbasic\nsetup\ninstructions.\n\n##\nBuilding\nSpark\n\nSpark\nis\nbuilt\nusing\n[Apache\nMaven](http://maven.apache.org/).\nTo\nbuild\nSpark\nand\nits\nexample\nprograms,\nrun:\n\n\n\n\n\nbuild/mvn\n-DskipTests\nclean\npackage\n\n(You\ndo\nnot\nneed\nto\ndo\nthis\nif\nyou\ndownloaded\na\npre-built\npackage.)\nMore\ndetailed\ndocumentation\nis\navailable\nfrom\nthe\nproject\nsite,\nat\n[&#34;Building\nSpark&#34;](http://spark.apache.org/docs/latest/building-spark.html).\n\n##\nInteractive\nScala\nShell\n\nThe\neasiest\nway\nto\nstart\nusing\nSpark\nis\nthrough\nthe\nScala\nshell:\n\n\n\n\n\n./bin/spark-shell\n\nTry\nthe\nfollowing\ncommand,\nwhich\nshould\nreturn\n1000:\n\n\n\n\n\nscala&gt;\nsc.parallelize(1\nto\n1000).count()\n\n##\nInteractive\nPython\nShell\n\nAlternatively,\nif\nyou\nprefer\nPython,\nyou\ncan\nuse\nthe\nPython\nshell:\n\n\n\n\n\n./bin/pyspark\n\nAnd\nrun\nthe\nfollowing\ncommand,\nwhich\nshould\nalso\nreturn\n1000:\n\n\n\n\n\n&gt;&gt;&gt;\nsc.parallelize(range(1000)).count()\n\n##\nExample\nPrograms\n\nSpark\nalso\ncomes\nwith\nseveral\nsample\nprograms\nin\nthe\n`examples`\ndirectory.\nTo\nrun\none\nof\nthem,\nuse\n`./bin/run-example\n&lt;class&gt;\n[params]`.\nFor\nexample:\n\n\n\n\n\n./bin/run-example\nSparkPi\n\nwill\nrun\nthe\nPi\nexample\nlocally.\n\nYou\ncan\nset\nthe\nMASTER\nenvironment\nvariable\nwhen\nrunning\nexamples\nto\nsubmit\nexamples\nto\na\ncluster.\nThis\ncan\nbe\na\nmesos://\nor\nspark://\nURL,\n&#34;yarn&#34;\nto\nrun\non\nYARN,\nand\n&#34;local&#34;\nto\nrun\nlocally\nwith\none\nthread,\nor\n&#34;local[N]&#34;\nto\nrun\nlocally\nwith\nN\nthreads.\nYou\ncan\nalso\nuse\nan\nabbreviated\nclass\nname\nif\nthe\nclass\nis\nin\nthe\n`examples`\npackage.\nFor\ninstance:\n\n\n\n\n\nMASTER=spark://host:7077\n./bin/run-example\nSparkPi\n\nMany\nof\nthe\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven.\n\n##\nRunning\nTests\n\nTesting\nfirst\nrequires\n[building\nSpark](#building-spark).\nOnce\nSpark\nis\nbuilt,\ntests\ncan\nbe\nrun\nusing:\n\n\n\n\n\n./dev/run-tests\n\nPlease\nsee\nthe\nguidance\non\nhow\nto\n[run\ntests\nfor\na\nmodule,\nor\nindividual\ntests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).\n\n##\nA\nNote\nAbout\nHadoop\nVersions\n\nSpark\nuses\nthe\nHadoop\ncore\nlibrary\nto\ntalk\nto\nHDFS\nand\nother\nHadoop-supported\nstorage\nsystems.\nBecause\nthe\nprotocols\nhave\nchanged\nin\ndifferent\nversions\nof\nHadoop,\nyou\nmust\nbuild\nSpark\nagainst\nthe\nsame\nversion\nthat\nyour\ncluster\nruns.\n\nPlease\nrefer\nto\nthe\nbuild\ndocumentation\nat\n[&#34;Specifying\nthe\nHadoop\nVersion&#34;](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nHadoop,\nincluding\nbuilding\nfor\nparticular\nHive\nand\nHive\nThriftserver\ndistributions.\n\n##\nConfiguration\n\nPlease\nrefer\nto\nthe\n[Configuration\nGuide](http://spark.apache.org/docs/latest/configuration.html)\nin\nthe\nonline\ndocumentation\nfor\nan\noverview\non\nhow\nto\nconfigure\nSpark.\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Take the previous function and\n# 1. count all the words\nrdd2=words.map(lambda x: (x,1))\nrdd3=rdd2.reduceByKey(lambda a,b: a+b)\nfor r in rdd3.collect():\n  print(r)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&#39;#&#39;, 1)\n(&#39;Apache&#39;, 1)\n(&#39;Spark&#39;, 13)\n(&#39;&#39;, 67)\n(&#39;is&#39;, 6)\n(&#39;It&#39;, 2)\n(&#39;provides&#39;, 1)\n(&#39;high-level&#39;, 1)\n(&#39;APIs&#39;, 1)\n(&#39;in&#39;, 5)\n(&#39;Scala,&#39;, 1)\n(&#39;Java,&#39;, 1)\n(&#39;an&#39;, 3)\n(&#39;optimized&#39;, 1)\n(&#39;engine&#39;, 1)\n(&#39;supports&#39;, 2)\n(&#39;computation&#39;, 1)\n(&#39;analysis.&#39;, 1)\n(&#39;set&#39;, 2)\n(&#39;of&#39;, 5)\n(&#39;tools&#39;, 1)\n(&#39;SQL&#39;, 2)\n(&#39;MLlib&#39;, 1)\n(&#39;machine&#39;, 1)\n(&#39;learning,&#39;, 1)\n(&#39;GraphX&#39;, 1)\n(&#39;graph&#39;, 1)\n(&#39;processing,&#39;, 1)\n(&#39;Documentation&#39;, 1)\n(&#39;latest&#39;, 1)\n(&#39;programming&#39;, 1)\n(&#39;guide,&#39;, 1)\n(&#39;[project&#39;, 2)\n(&#39;README&#39;, 1)\n(&#39;only&#39;, 1)\n(&#39;basic&#39;, 1)\n(&#39;instructions.&#39;, 1)\n(&#39;Building&#39;, 1)\n(&#39;using&#39;, 2)\n(&#39;[Apache&#39;, 1)\n(&#39;run:&#39;, 1)\n(&#39;do&#39;, 2)\n(&#39;this&#39;, 1)\n(&#39;downloaded&#39;, 1)\n(&#39;documentation&#39;, 3)\n(&#39;project&#39;, 1)\n(&#39;site,&#39;, 1)\n(&#39;at&#39;, 2)\n(&#39;Spark&#34;](http://spark.apache.org/docs/latest/building-spark.html).&#39;, 1)\n(&#39;Interactive&#39;, 2)\n(&#39;Shell&#39;, 2)\n(&#39;The&#39;, 1)\n(&#39;way&#39;, 1)\n(&#39;start&#39;, 1)\n(&#39;Try&#39;, 1)\n(&#39;following&#39;, 2)\n(&#39;1000:&#39;, 2)\n(&#39;scala&gt;&#39;, 1)\n(&#39;1000).count()&#39;, 1)\n(&#39;Python&#39;, 2)\n(&#39;Alternatively,&#39;, 1)\n(&#39;use&#39;, 3)\n(&#39;And&#39;, 1)\n(&#39;run&#39;, 7)\n(&#39;Example&#39;, 1)\n(&#39;several&#39;, 1)\n(&#39;programs&#39;, 2)\n(&#39;them,&#39;, 1)\n(&#39;`./bin/run-example&#39;, 1)\n(&#39;[params]`.&#39;, 1)\n(&#39;example:&#39;, 1)\n(&#39;./bin/run-example&#39;, 2)\n(&#39;SparkPi&#39;, 2)\n(&#39;variable&#39;, 1)\n(&#39;when&#39;, 1)\n(&#39;examples&#39;, 2)\n(&#39;spark://&#39;, 1)\n(&#39;URL,&#39;, 1)\n(&#39;YARN,&#39;, 1)\n(&#39;&#34;local&#34;&#39;, 1)\n(&#39;locally&#39;, 2)\n(&#39;N&#39;, 1)\n(&#39;abbreviated&#39;, 1)\n(&#39;class&#39;, 2)\n(&#39;name&#39;, 1)\n(&#39;package.&#39;, 1)\n(&#39;instance:&#39;, 1)\n(&#39;print&#39;, 1)\n(&#39;usage&#39;, 1)\n(&#39;help&#39;, 1)\n(&#39;no&#39;, 1)\n(&#39;params&#39;, 1)\n(&#39;are&#39;, 1)\n(&#39;Testing&#39;, 1)\n(&#39;Spark](#building-spark).&#39;, 1)\n(&#39;Once&#39;, 1)\n(&#39;built,&#39;, 1)\n(&#39;tests&#39;, 2)\n(&#39;using:&#39;, 1)\n(&#39;./dev/run-tests&#39;, 1)\n(&#39;Please&#39;, 3)\n(&#39;guidance&#39;, 2)\n(&#39;module,&#39;, 1)\n(&#39;individual&#39;, 1)\n(&#39;Note&#39;, 1)\n(&#39;About&#39;, 1)\n(&#39;uses&#39;, 1)\n(&#39;library&#39;, 1)\n(&#39;HDFS&#39;, 1)\n(&#39;other&#39;, 1)\n(&#39;Hadoop-supported&#39;, 1)\n(&#39;storage&#39;, 1)\n(&#39;systems.&#39;, 1)\n(&#39;Because&#39;, 1)\n(&#39;have&#39;, 1)\n(&#39;changed&#39;, 1)\n(&#39;different&#39;, 1)\n(&#39;versions&#39;, 1)\n(&#39;Hadoop,&#39;, 2)\n(&#39;must&#39;, 1)\n(&#39;against&#39;, 1)\n(&#39;version&#39;, 1)\n(&#39;refer&#39;, 2)\n(&#39;particular&#39;, 2)\n(&#39;distribution&#39;, 1)\n(&#39;Hive&#39;, 2)\n(&#39;Thriftserver&#39;, 1)\n(&#39;distributions.&#39;, 1)\n(&#39;[Configuration&#39;, 1)\n(&#39;Guide](http://spark.apache.org/docs/latest/configuration.html)&#39;, 1)\n(&#39;online&#39;, 1)\n(&#39;overview&#39;, 1)\n(&#39;configure&#39;, 1)\n(&#39;Spark.&#39;, 1)\n(&#39;a&#39;, 8)\n(&#39;fast&#39;, 1)\n(&#39;and&#39;, 10)\n(&#39;general&#39;, 2)\n(&#39;cluster&#39;, 2)\n(&#39;computing&#39;, 1)\n(&#39;system&#39;, 1)\n(&#39;for&#39;, 11)\n(&#39;Big&#39;, 1)\n(&#39;Data.&#39;, 1)\n(&#39;Python,&#39;, 2)\n(&#39;R,&#39;, 1)\n(&#39;that&#39;, 2)\n(&#39;graphs&#39;, 1)\n(&#39;data&#39;, 1)\n(&#39;also&#39;, 4)\n(&#39;rich&#39;, 1)\n(&#39;higher-level&#39;, 1)\n(&#39;including&#39;, 3)\n(&#39;DataFrames,&#39;, 1)\n(&#39;Streaming&#39;, 1)\n(&#39;stream&#39;, 1)\n(&#39;processing.&#39;, 1)\n(&#39;&lt;http://spark.apache.org/&gt;&#39;, 1)\n(&#39;##&#39;, 8)\n(&#39;Online&#39;, 1)\n(&#39;You&#39;, 3)\n(&#39;can&#39;, 6)\n(&#39;find&#39;, 1)\n(&#39;the&#39;, 21)\n(&#39;documentation,&#39;, 1)\n(&#39;on&#39;, 5)\n(&#39;web&#39;, 1)\n(&#39;page](http://spark.apache.org/documentation.html)&#39;, 1)\n(&#39;wiki](https://cwiki.apache.org/confluence/display/SPARK).&#39;, 1)\n(&#39;This&#39;, 2)\n(&#39;file&#39;, 1)\n(&#39;contains&#39;, 1)\n(&#39;setup&#39;, 1)\n(&#39;built&#39;, 1)\n(&#39;Maven](http://maven.apache.org/).&#39;, 1)\n(&#39;To&#39;, 2)\n(&#39;build&#39;, 3)\n(&#39;its&#39;, 1)\n(&#39;example&#39;, 3)\n(&#39;programs,&#39;, 1)\n(&#39;build/mvn&#39;, 1)\n(&#39;-DskipTests&#39;, 1)\n(&#39;clean&#39;, 1)\n(&#39;package&#39;, 1)\n(&#39;(You&#39;, 1)\n(&#39;not&#39;, 1)\n(&#39;need&#39;, 1)\n(&#39;to&#39;, 14)\n(&#39;if&#39;, 4)\n(&#39;you&#39;, 4)\n(&#39;pre-built&#39;, 1)\n(&#39;package.)&#39;, 1)\n(&#39;More&#39;, 1)\n(&#39;detailed&#39;, 2)\n(&#39;available&#39;, 1)\n(&#39;from&#39;, 1)\n(&#39;[&#34;Building&#39;, 1)\n(&#39;Scala&#39;, 2)\n(&#39;easiest&#39;, 1)\n(&#39;through&#39;, 1)\n(&#39;shell:&#39;, 2)\n(&#39;./bin/spark-shell&#39;, 1)\n(&#39;command,&#39;, 2)\n(&#39;which&#39;, 2)\n(&#39;should&#39;, 2)\n(&#39;return&#39;, 2)\n(&#39;sc.parallelize(1&#39;, 1)\n(&#39;prefer&#39;, 1)\n(&#39;./bin/pyspark&#39;, 1)\n(&#39;&gt;&gt;&gt;&#39;, 1)\n(&#39;sc.parallelize(range(1000)).count()&#39;, 1)\n(&#39;Programs&#39;, 1)\n(&#39;comes&#39;, 1)\n(&#39;with&#39;, 3)\n(&#39;sample&#39;, 1)\n(&#39;`examples`&#39;, 2)\n(&#39;directory.&#39;, 1)\n(&#39;one&#39;, 2)\n(&#39;&lt;class&gt;&#39;, 1)\n(&#39;For&#39;, 2)\n(&#39;will&#39;, 1)\n(&#39;Pi&#39;, 1)\n(&#39;locally.&#39;, 1)\n(&#39;MASTER&#39;, 1)\n(&#39;environment&#39;, 1)\n(&#39;running&#39;, 1)\n(&#39;submit&#39;, 1)\n(&#39;cluster.&#39;, 1)\n(&#39;be&#39;, 2)\n(&#39;mesos://&#39;, 1)\n(&#39;or&#39;, 3)\n(&#39;&#34;yarn&#34;&#39;, 1)\n(&#39;thread,&#39;, 1)\n(&#39;&#34;local[N]&#34;&#39;, 1)\n(&#39;threads.&#39;, 1)\n(&#39;MASTER=spark://host:7077&#39;, 1)\n(&#39;Many&#39;, 1)\n(&#39;given.&#39;, 1)\n(&#39;Running&#39;, 1)\n(&#39;Tests&#39;, 1)\n(&#39;first&#39;, 1)\n(&#39;requires&#39;, 1)\n(&#39;[building&#39;, 1)\n(&#39;see&#39;, 1)\n(&#39;how&#39;, 2)\n(&#39;[run&#39;, 1)\n(&#39;tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).&#39;, 1)\n(&#39;A&#39;, 1)\n(&#39;Hadoop&#39;, 3)\n(&#39;Versions&#39;, 1)\n(&#39;core&#39;, 1)\n(&#39;talk&#39;, 1)\n(&#39;protocols&#39;, 1)\n(&#39;same&#39;, 1)\n(&#39;your&#39;, 1)\n(&#39;runs.&#39;, 1)\n(&#39;[&#34;Specifying&#39;, 1)\n(&#39;Version&#34;](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)&#39;, 1)\n(&#39;building&#39;, 2)\n(&#39;Configuration&#39;, 1)\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# 2. change all capital letters to lower case\ndef Func(lines):\n  lines = lines.lower()\n  lines = lines.split()\n  return lines\nrdd4 = words.map(Func)\nrdd4.take(20)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[63]: [[&#39;#&#39;],\n [&#39;apache&#39;],\n [&#39;spark&#39;],\n [],\n [&#39;spark&#39;],\n [&#39;is&#39;],\n [&#39;a&#39;],\n [&#39;fast&#39;],\n [&#39;and&#39;],\n [&#39;general&#39;],\n [&#39;cluster&#39;],\n [&#39;computing&#39;],\n [&#39;system&#39;],\n [&#39;for&#39;],\n [&#39;big&#39;],\n [&#39;data.&#39;],\n [&#39;it&#39;],\n [&#39;provides&#39;],\n [&#39;high-level&#39;],\n [&#39;apis&#39;]]</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# 3. eliminate stopwords \nstopwords = ['is','am','are','the','for','a']\nrdd5 = rdd4.filter(lambda x: x not in stopwords)\nrdd5.take(10)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[66]: [[&#39;#&#39;],\n [&#39;apache&#39;],\n [&#39;spark&#39;],\n [],\n [&#39;spark&#39;],\n [&#39;is&#39;],\n [&#39;a&#39;],\n [&#39;fast&#39;],\n [&#39;and&#39;],\n [&#39;general&#39;]]</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# 4. sort in alphabetical order\nrdd6=rdd3.sortByKey()\nfor i in rdd6.collect():\n     print (i)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&#39;&#39;, 67)\n(&#39;&#34;local&#34;&#39;, 1)\n(&#39;&#34;local[N]&#34;&#39;, 1)\n(&#39;&#34;yarn&#34;&#39;, 1)\n(&#39;#&#39;, 1)\n(&#39;##&#39;, 8)\n(&#39;(You&#39;, 1)\n(&#39;-DskipTests&#39;, 1)\n(&#39;./bin/pyspark&#39;, 1)\n(&#39;./bin/run-example&#39;, 2)\n(&#39;./bin/spark-shell&#39;, 1)\n(&#39;./dev/run-tests&#39;, 1)\n(&#39;1000).count()&#39;, 1)\n(&#39;1000:&#39;, 2)\n(&#39;&lt;class&gt;&#39;, 1)\n(&#39;&lt;http://spark.apache.org/&gt;&#39;, 1)\n(&#39;&gt;&gt;&gt;&#39;, 1)\n(&#39;A&#39;, 1)\n(&#39;APIs&#39;, 1)\n(&#39;About&#39;, 1)\n(&#39;Alternatively,&#39;, 1)\n(&#39;And&#39;, 1)\n(&#39;Apache&#39;, 1)\n(&#39;Because&#39;, 1)\n(&#39;Big&#39;, 1)\n(&#39;Building&#39;, 1)\n(&#39;Configuration&#39;, 1)\n(&#39;Data.&#39;, 1)\n(&#39;DataFrames,&#39;, 1)\n(&#39;Documentation&#39;, 1)\n(&#39;Example&#39;, 1)\n(&#39;For&#39;, 2)\n(&#39;GraphX&#39;, 1)\n(&#39;Guide](http://spark.apache.org/docs/latest/configuration.html)&#39;, 1)\n(&#39;HDFS&#39;, 1)\n(&#39;Hadoop&#39;, 3)\n(&#39;Hadoop,&#39;, 2)\n(&#39;Hadoop-supported&#39;, 1)\n(&#39;Hive&#39;, 2)\n(&#39;Interactive&#39;, 2)\n(&#39;It&#39;, 2)\n(&#39;Java,&#39;, 1)\n(&#39;MASTER&#39;, 1)\n(&#39;MASTER=spark://host:7077&#39;, 1)\n(&#39;MLlib&#39;, 1)\n(&#39;Many&#39;, 1)\n(&#39;Maven](http://maven.apache.org/).&#39;, 1)\n(&#39;More&#39;, 1)\n(&#39;N&#39;, 1)\n(&#39;Note&#39;, 1)\n(&#39;Once&#39;, 1)\n(&#39;Online&#39;, 1)\n(&#39;Pi&#39;, 1)\n(&#39;Please&#39;, 3)\n(&#39;Programs&#39;, 1)\n(&#39;Python&#39;, 2)\n(&#39;Python,&#39;, 2)\n(&#39;R,&#39;, 1)\n(&#39;README&#39;, 1)\n(&#39;Running&#39;, 1)\n(&#39;SQL&#39;, 2)\n(&#39;Scala&#39;, 2)\n(&#39;Scala,&#39;, 1)\n(&#39;Shell&#39;, 2)\n(&#39;Spark&#39;, 13)\n(&#39;Spark&#34;](http://spark.apache.org/docs/latest/building-spark.html).&#39;, 1)\n(&#39;Spark.&#39;, 1)\n(&#39;SparkPi&#39;, 2)\n(&#39;Spark](#building-spark).&#39;, 1)\n(&#39;Streaming&#39;, 1)\n(&#39;Testing&#39;, 1)\n(&#39;Tests&#39;, 1)\n(&#39;The&#39;, 1)\n(&#39;This&#39;, 2)\n(&#39;Thriftserver&#39;, 1)\n(&#39;To&#39;, 2)\n(&#39;Try&#39;, 1)\n(&#39;URL,&#39;, 1)\n(&#39;Version&#34;](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)&#39;, 1)\n(&#39;Versions&#39;, 1)\n(&#39;YARN,&#39;, 1)\n(&#39;You&#39;, 3)\n(&#39;[&#34;Building&#39;, 1)\n(&#39;[&#34;Specifying&#39;, 1)\n(&#39;[Apache&#39;, 1)\n(&#39;[Configuration&#39;, 1)\n(&#39;[building&#39;, 1)\n(&#39;[params]`.&#39;, 1)\n(&#39;[project&#39;, 2)\n(&#39;[run&#39;, 1)\n(&#39;`./bin/run-example&#39;, 1)\n(&#39;`examples`&#39;, 2)\n(&#39;a&#39;, 8)\n(&#39;abbreviated&#39;, 1)\n(&#39;against&#39;, 1)\n(&#39;also&#39;, 4)\n(&#39;an&#39;, 3)\n(&#39;analysis.&#39;, 1)\n(&#39;and&#39;, 10)\n(&#39;are&#39;, 1)\n(&#39;at&#39;, 2)\n(&#39;available&#39;, 1)\n(&#39;basic&#39;, 1)\n(&#39;be&#39;, 2)\n(&#39;build&#39;, 3)\n(&#39;build/mvn&#39;, 1)\n(&#39;building&#39;, 2)\n(&#39;built&#39;, 1)\n(&#39;built,&#39;, 1)\n(&#39;can&#39;, 6)\n(&#39;changed&#39;, 1)\n(&#39;class&#39;, 2)\n(&#39;clean&#39;, 1)\n(&#39;cluster&#39;, 2)\n(&#39;cluster.&#39;, 1)\n(&#39;comes&#39;, 1)\n(&#39;command,&#39;, 2)\n(&#39;computation&#39;, 1)\n(&#39;computing&#39;, 1)\n(&#39;configure&#39;, 1)\n(&#39;contains&#39;, 1)\n(&#39;core&#39;, 1)\n(&#39;data&#39;, 1)\n(&#39;detailed&#39;, 2)\n(&#39;different&#39;, 1)\n(&#39;directory.&#39;, 1)\n(&#39;distribution&#39;, 1)\n(&#39;distributions.&#39;, 1)\n(&#39;do&#39;, 2)\n(&#39;documentation&#39;, 3)\n(&#39;documentation,&#39;, 1)\n(&#39;downloaded&#39;, 1)\n(&#39;easiest&#39;, 1)\n(&#39;engine&#39;, 1)\n(&#39;environment&#39;, 1)\n(&#39;example&#39;, 3)\n(&#39;example:&#39;, 1)\n(&#39;examples&#39;, 2)\n(&#39;fast&#39;, 1)\n(&#39;file&#39;, 1)\n(&#39;find&#39;, 1)\n(&#39;first&#39;, 1)\n(&#39;following&#39;, 2)\n(&#39;for&#39;, 11)\n(&#39;from&#39;, 1)\n(&#39;general&#39;, 2)\n(&#39;given.&#39;, 1)\n(&#39;graph&#39;, 1)\n(&#39;graphs&#39;, 1)\n(&#39;guidance&#39;, 2)\n(&#39;guide,&#39;, 1)\n(&#39;have&#39;, 1)\n(&#39;help&#39;, 1)\n(&#39;high-level&#39;, 1)\n(&#39;higher-level&#39;, 1)\n(&#39;how&#39;, 2)\n(&#39;if&#39;, 4)\n(&#39;in&#39;, 5)\n(&#39;including&#39;, 3)\n(&#39;individual&#39;, 1)\n(&#39;instance:&#39;, 1)\n(&#39;instructions.&#39;, 1)\n(&#39;is&#39;, 6)\n(&#39;its&#39;, 1)\n(&#39;latest&#39;, 1)\n(&#39;learning,&#39;, 1)\n(&#39;library&#39;, 1)\n(&#39;locally&#39;, 2)\n(&#39;locally.&#39;, 1)\n(&#39;machine&#39;, 1)\n(&#39;mesos://&#39;, 1)\n(&#39;module,&#39;, 1)\n(&#39;must&#39;, 1)\n(&#39;name&#39;, 1)\n(&#39;need&#39;, 1)\n(&#39;no&#39;, 1)\n(&#39;not&#39;, 1)\n(&#39;of&#39;, 5)\n(&#39;on&#39;, 5)\n(&#39;one&#39;, 2)\n(&#39;online&#39;, 1)\n(&#39;only&#39;, 1)\n(&#39;optimized&#39;, 1)\n(&#39;or&#39;, 3)\n(&#39;other&#39;, 1)\n(&#39;overview&#39;, 1)\n(&#39;package&#39;, 1)\n(&#39;package.&#39;, 1)\n(&#39;package.)&#39;, 1)\n(&#39;page](http://spark.apache.org/documentation.html)&#39;, 1)\n(&#39;params&#39;, 1)\n(&#39;particular&#39;, 2)\n(&#39;pre-built&#39;, 1)\n(&#39;prefer&#39;, 1)\n(&#39;print&#39;, 1)\n(&#39;processing,&#39;, 1)\n(&#39;processing.&#39;, 1)\n(&#39;programming&#39;, 1)\n(&#39;programs&#39;, 2)\n(&#39;programs,&#39;, 1)\n(&#39;project&#39;, 1)\n(&#39;protocols&#39;, 1)\n(&#39;provides&#39;, 1)\n(&#39;refer&#39;, 2)\n(&#39;requires&#39;, 1)\n(&#39;return&#39;, 2)\n(&#39;rich&#39;, 1)\n(&#39;run&#39;, 7)\n(&#39;run:&#39;, 1)\n(&#39;running&#39;, 1)\n(&#39;runs.&#39;, 1)\n(&#39;same&#39;, 1)\n(&#39;sample&#39;, 1)\n(&#39;sc.parallelize(1&#39;, 1)\n(&#39;sc.parallelize(range(1000)).count()&#39;, 1)\n(&#39;scala&gt;&#39;, 1)\n(&#39;see&#39;, 1)\n(&#39;set&#39;, 2)\n(&#39;setup&#39;, 1)\n(&#39;several&#39;, 1)\n(&#39;shell:&#39;, 2)\n(&#39;should&#39;, 2)\n(&#39;site,&#39;, 1)\n(&#39;spark://&#39;, 1)\n(&#39;start&#39;, 1)\n(&#39;storage&#39;, 1)\n(&#39;stream&#39;, 1)\n(&#39;submit&#39;, 1)\n(&#39;supports&#39;, 2)\n(&#39;system&#39;, 1)\n(&#39;systems.&#39;, 1)\n(&#39;talk&#39;, 1)\n(&#39;tests&#39;, 2)\n(&#39;tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).&#39;, 1)\n(&#39;that&#39;, 2)\n(&#39;the&#39;, 21)\n(&#39;them,&#39;, 1)\n(&#39;this&#39;, 1)\n(&#39;thread,&#39;, 1)\n(&#39;threads.&#39;, 1)\n(&#39;through&#39;, 1)\n(&#39;to&#39;, 14)\n(&#39;tools&#39;, 1)\n(&#39;usage&#39;, 1)\n(&#39;use&#39;, 3)\n(&#39;uses&#39;, 1)\n(&#39;using&#39;, 2)\n(&#39;using:&#39;, 1)\n(&#39;variable&#39;, 1)\n(&#39;version&#39;, 1)\n(&#39;versions&#39;, 1)\n(&#39;way&#39;, 1)\n(&#39;web&#39;, 1)\n(&#39;when&#39;, 1)\n(&#39;which&#39;, 2)\n(&#39;wiki](https://cwiki.apache.org/confluence/display/SPARK).&#39;, 1)\n(&#39;will&#39;, 1)\n(&#39;with&#39;, 3)\n(&#39;you&#39;, 4)\n(&#39;your&#39;, 1)\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["!pip install nltk"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# 5. sort from most to least frequent word\nrdd7 = rdd6.map(lambda x:(x[1],x[0]))\nrdd7.sortByKey(False).take(20)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[59]: [(67, &#39;&#39;),\n (21, &#39;the&#39;),\n (14, &#39;to&#39;),\n (13, &#39;Spark&#39;),\n (11, &#39;for&#39;),\n (10, &#39;and&#39;),\n (8, &#39;##&#39;),\n (8, &#39;a&#39;),\n (7, &#39;run&#39;),\n (6, &#39;can&#39;),\n (6, &#39;is&#39;),\n (5, &#39;in&#39;),\n (5, &#39;of&#39;),\n (5, &#39;on&#39;),\n (4, &#39;also&#39;),\n (4, &#39;if&#39;),\n (4, &#39;you&#39;),\n (3, &#39;Hadoop&#39;),\n (3, &#39;Please&#39;),\n (3, &#39;You&#39;)]</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# 6.** remove punctuations \ndef lower_clean_str(x):\n  punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n  lowercased_str = x.lower()\n  for ch in punc:\n    lowercased_str = lowercased_str.replace(ch, '')\n  return lowercased_str\nrdd = rdd.map(lower_clean_str)\nrdd=rdd.flatMap(lambda x: x.split(\" \"))\n#filter white spaces\nrdd = rdd.filter(lambda x:x!='')\n#Count how many times each word occurs\ncount=rdd.map(lambda  word:(word,1))\n#Apply ReduceByKey to find frequent words\ncount_RBK=count.reduceByKey(lambda x,y:(x+y)).sortByKey()\n#We want to sort the most frequent words in descending order. As the first step, we switch (key,val) pairs as (val,key).\ncount_RBK=count_RBK.map(lambda x:(x[1],x[0]))\n#We see that the most common word is \"the\". However, these values are words that we call stopwords which brings value to our analysis\ncount_RBK.sortByKey(False).take(10)\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords =stopwords.words('english')\ncount_RBK = count_RBK.filter(lambda x: x[1] not in stopwords).sortByKey(False)\ncount_RBK.sortByKey(False).take(20)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nOut[82]: [(15, &#39;spark&#39;),\n (9, &#39;run&#39;),\n (5, &#39;building&#39;),\n (5, &#39;documentation&#39;),\n (5, &#39;example&#39;),\n (5, &#39;hadoop&#39;),\n (4, &#39;also&#39;),\n (4, &#39;examples&#39;),\n (4, &#39;programs&#39;),\n (4, &#39;python&#39;),\n (4, &#39;scala&#39;),\n (4, &#39;shell&#39;),\n (3, &#39;binrunexample&#39;),\n (3, &#39;build&#39;),\n (3, &#39;class&#39;),\n (3, &#39;cluster&#39;),\n (3, &#39;including&#39;),\n (3, &#39;locally&#39;),\n (3, &#39;package&#39;),\n (3, &#39;please&#39;)]</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Create an RDD of tuples (name, age)\ndataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n(\"TD\", 35), (\"Brooke\", 25)])\n\n# Try to undestand what this code does (line by line)\nagesRDD = (dataRDD\n  .map(lambda x: (x[0], (x[1], 1)))\n  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n  .map(lambda x: (x[0], x[1][0]/x[1][1])))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"Lab1_RDD_excercises","notebookId":798584523736014},"nbformat":4,"nbformat_minor":0}
